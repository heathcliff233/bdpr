diff --git a/__pycache__/data.cpython-38.pyc b/__pycache__/data.cpython-38.pyc
index f388ec7..3c3c696 100644
Binary files a/__pycache__/data.cpython-38.pyc and b/__pycache__/data.cpython-38.pyc differ
diff --git a/__pycache__/model.cpython-38.pyc b/__pycache__/model.cpython-38.pyc
index 5d2f205..8b15387 100644
Binary files a/__pycache__/model.cpython-38.pyc and b/__pycache__/model.cpython-38.pyc differ
diff --git a/__pycache__/train.cpython-38.pyc b/__pycache__/train.cpython-38.pyc
index a962734..8cb26c8 100644
Binary files a/__pycache__/train.cpython-38.pyc and b/__pycache__/train.cpython-38.pyc differ
diff --git a/data.py b/data.py
index 1845af6..b52fb04 100644
--- a/data.py
+++ b/data.py
@@ -8,15 +8,17 @@ import random
 import subprocess
 DISTRIBUTED = True
 class  MyDataset(Dataset):
-    def __init__(self, root: str, is_train: bool):
+    def __init__(self, root: str, is_train: bool, names: str, lines: List[int]):
         self.root = root
-        self.names, self.lines = self.get_filename(root)
+        self.names = names
+        self.lines = lines
+        #self.names, self.lines = self.get_filename(root)
         self.is_train = is_train
 
     def wc_count(self, file_name):
-        #out = subprocess.getoutput("wc -l %s" % file_name)
-        #return int(out.split()[0])
-        return 200
+        out = subprocess.getoutput("wc -l %s" % file_name)
+        return int(out.split()[0])
+        #return 200
     
     def get_filename(self, path: str) -> List[str]:
         files = os.listdir(path)
@@ -33,7 +35,7 @@ class  MyDataset(Dataset):
         if self.is_train:
             span = range(int(lines*0.8))
         else:
-            span = range(int(lines*0.8,lines))
+            span = range(int(lines*0.8),lines)
         idx1, idx2 = random.sample(span, 2)
         seq1 = re.sub('[(a-z)(-)]', '', linecache.getline(path, 2*idx1 + 2))
         seq2 = re.sub('[(a-z)(-)]', '', linecache.getline(path, 2*idx2 + 2))
diff --git a/main.py b/main.py
index 304cff9..ff5fd5b 100644
--- a/main.py
+++ b/main.py
@@ -1,3 +1,8 @@
+import pickle
+import wandb
+import subprocess
+import os
+from typing import Sequence, Tuple, List, Union
 import esm
 import torch
 import torch.nn as nn
@@ -9,17 +14,47 @@ from data import MyDataset, BatchConverter, DistributedProxySampler
 from train import train, evaluate
 
 DISTRIBUTED = True
-BATCHSZ = 16
+TRBATCHSZ = 16
+EVBATCHSZ = 16
 use_wandb = False
 threshold = 0.7
 eval_per_step = 40
 lr = 1e-5
+use_wandb = True
 path = "/share/wangsheng/train_test_data/cath35_20201021/cath35_a3m/"
 
+def wc_count(file_name):
+    return 200
+    #out = subprocess.getoutput("wc -l %s" % file_name)
+    #return int(out.split()[0])
+    
+def get_filename(path: str) -> List[str]:
+    files = os.listdir(path)
+    names = []
+    lines = []
+    for file in files:
+        if ".a3m" in file:
+            names.append(path + file)
+    lines = [wc_count(name) for name in names]
+
+    return names, lines
+
+def init_wandb():
+    wandb.init(
+        project="Retrieval",
+        config= {
+            "optim" : "AdamW",
+            "lr" : lr,
+            "train_batch" : TRBATCHSZ,
+            "eval_per_step" : EVBATCHSZ,
+        }
+    )
+
+
 if __name__ == "__main__":
     encoder, alphabet = esm.pretrained.esm1_t6_43M_UR50S()
-    #print(alphabet.__dict__)
     print("loaded model")
+    
     model = MyEncoder(encoder, 0)
     device = torch.device("cuda:0")
     if DISTRIBUTED:
@@ -29,11 +64,20 @@ if __name__ == "__main__":
         torch.cuda.set_device(local_rank)
         device = torch.device("cuda", local_rank)
         #model = nn.DataParallel(model, device_ids)
+
+    if use_wandb:
+        if DISTRIBUTED:
+            if torch.distributed.get_rank()==0:
+                init_wandb()
+        else:
+            init_wandb()
+    
     model = model.to(device)
-    train_set = MyDataset(path, True)
-    eval_set = MyDataset(path, False)
-    trbatch = train_set.get_batch_indices(BATCHSZ)
-    evbatch = train_set.get_batch_indices(BATCHSZ)
+    names, lines = get_filename(path)
+    train_set = MyDataset(path, True, names, lines)
+    eval_set = MyDataset(path, False, names, lines)
+    trbatch = train_set.get_batch_indices(TRBATCHSZ)
+    evbatch = train_set.get_batch_indices(EVBATCHSZ)
     if DISTRIBUTED:
         model = torch.nn.parallel.DistributedDataParallel(
             model,
@@ -50,4 +94,17 @@ if __name__ == "__main__":
     print("loaded dataset")
     optimizer = AdamW(model.parameters(), lr=lr)
     scheduler = lr_scheduler.StepLR(optimizer,step_size=40,gamma = 0.85)
-    train(model, train_loader, eval_loader, n_epoches=20, optimizer=optimizer, threshold=threshold, eval_per_step=eval_per_step, use_wandb=use_wandb, use_distr=DISTRIBUTED, device=device, acc_step=4)
+    train(
+        model, 
+        train_loader, 
+        eval_loader, 
+        n_epoches=20, 
+        optimizer=optimizer, 
+        threshold=threshold, 
+        eval_per_step=eval_per_step, 
+        use_wandb=use_wandb, 
+        use_distr=DISTRIBUTED, 
+        device=device, 
+        acc_step=4
+    )
+
diff --git a/model.py b/model.py
index 31bc035..94d51d8 100644
--- a/model.py
+++ b/model.py
@@ -30,7 +30,7 @@ class MyEncoder(nn.Module):
 
     def get_loss(self, ebd):
         qebd, cebd = ebd 
-        print("q/c vec shape :", qebd.shape, cebd.shape)
+        #print("q/c vec shape :", qebd.shape, cebd.shape)
         sim_mx = dot_product_scores(qebd, cebd)
         #label = torch.eye(sim_mx.shape[0]).long()
         label = torch.arange(sim_mx.shape[0], dtype=torch.long)
@@ -39,13 +39,24 @@ class MyEncoder(nn.Module):
         correct_predictions_count = (
             max_idxs == label.to(sm_score.device)
         ).sum()
-        print("prediction ", correct_predictions_count.detach().cpu().item(), sim_mx.shape[0])
+        #print("prediction ", correct_predictions_count.detach().cpu().item(), sim_mx.shape[0])
         loss = F.nll_loss(
             sm_score,
             label.to(sm_score.device),
             reduction="mean"
         )
         return loss
+    
+    def get_acc(self, ebd):
+        qebd, cebd = ebd 
+        sim_mx = dot_product_scores(qebd, cebd)
+        label = torch.arange(sim_mx.shape[0], dtype=torch.long)
+        sm_score = F.log_softmax(sim_mx, dim=1)
+        max_score, max_idxs = torch.max(sm_score, 1)
+        correct_predictions_count = (
+            max_idxs == label.to(sm_score.device)
+        ).sum()
+        return correct_predictions_count, sim_mx.shape[0]
 
 
 def dot_product_scores(q_vectors, ctx_vectors):
diff --git a/train.py b/train.py
index 226e672..edef471 100644
--- a/train.py
+++ b/train.py
@@ -5,9 +5,14 @@ from torch.cuda.amp import GradScaler as GradScaler
 
 def train(model, train_loader, eval_loader, n_epoches, optimizer, threshold=0.7, eval_per_step=10, use_wandb=False, use_distr=True, device="cuda:0", acc_step=1):
     if use_wandb:
-        wandb.watch(model, log_freq=eval_per_step)
+        if use_distr:
+            if torch.distributed.get_rank()==0:
+                wandb.watch(model, log_freq=eval_per_step)
+        else:
+            wandb.watch(model, log_freq=eval_per_step)
     if use_distr:
         scaler = GradScaler()
+    save(model, -1)
     for epoch in range(n_epoches):
         print("epoch " + str(epoch+1))
         cnt = 0
@@ -32,11 +37,17 @@ def train(model, train_loader, eval_loader, n_epoches, optimizer, threshold=0.7,
             
             if cnt%eval_per_step == 0 :
                 if cnt%(eval_per_step*1)==0:
-                    acc = evaluate(model, eval_loader, threshold)
-                    ac2 = evaluate(model, train_loader,threshold)
+                    acc = evaluate(model, eval_loader, threshold, use_distr=use_distr)
+                    ac2 = evaluate(model, train_loader,threshold, use_distr=use_distr)
                     print("loss"+str(tot_loss/eval_per_step))
+                    acc = acc.view(-1).cpu().item()
+                    print("acc: ", acc)
                     if use_wandb :
-                        wandb.log({"train/train-acc": ac2, "train/eval-acc": acc,"train/loss": tot_loss/eval_per_step})
+                        if use_distr:
+                            if torch.distributed.get_rank()==0:
+                                wandb.log({"train/train-acc": ac2, "train/eval-acc": acc,"train/loss": tot_loss/eval_per_step})
+                        else :
+                            wandb.log({"train/train-acc": ac2, "train/eval-acc": acc,"train/loss": tot_loss/eval_per_step}) 
                     tot_loss = 0
                 model.train()
             
@@ -70,8 +81,30 @@ def train(model, train_loader, eval_loader, n_epoches, optimizer, threshold=0.7,
             #print(p, n)
             '''
             #optimizer.step()
-        #save(model, epoch)
+        save(model, epoch)
 
 
-def evaluate(model, loader, threshold):
-    return 0
+def evaluate(model, loader, threshold, use_distr=False):
+    model.eval()
+    correct = torch.tensor([0]).cuda()
+    total = torch.tensor([0]).cuda()
+    for i, (toks1, toks2) in enumerate(loader):
+        if i>20:
+            break
+        if use_distr:
+            with torch.no_grad():
+                out = model((toks1, toks2))
+            right, num = model.module.get_acc(out)
+            correct += right
+            total += num
+        else:
+            right, num = model.get_acc(model((toks1, toks2)))
+            correct += right
+            total += num
+    torch.distributed.all_reduce(correct, op=torch.distributed.ReduceOp.SUM)
+    torch.distributed.all_reduce(total, op=torch.distributed.ReduceOp.SUM)
+    return correct / total
+
+
+def save(model, epoch):
+    torch.save(model.state_dict(), './store/'+str(epoch)+'.pth')
