{"format": "torch", "nodes": [{"name": "module", "id": 139773815864864, "class_name": "MyEncoder(\n  (bert): ProteinBertModel(\n    (embed_tokens): Embedding(35, 768, padding_idx=1)\n    (layers): ModuleList(\n      (0): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (self_attn_layer_norm): ESM1LayerNorm()\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (final_layer_norm): ESM1LayerNorm()\n      )\n      (1): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (self_attn_layer_norm): ESM1LayerNorm()\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (final_layer_norm): ESM1LayerNorm()\n      )\n      (2): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (self_attn_layer_norm): ESM1LayerNorm()\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (final_layer_norm): ESM1LayerNorm()\n      )\n      (3): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (self_attn_layer_norm): ESM1LayerNorm()\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (final_layer_norm): ESM1LayerNorm()\n      )\n      (4): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (self_attn_layer_norm): ESM1LayerNorm()\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (final_layer_norm): ESM1LayerNorm()\n      )\n      (5): TransformerLayer(\n        (self_attn): MultiheadAttention(\n          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (self_attn_layer_norm): ESM1LayerNorm()\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (final_layer_norm): ESM1LayerNorm()\n      )\n    )\n    (contact_head): ContactPredictionHead(\n      (regression): Linear(in_features=72, out_features=1, bias=True)\n      (activation): Sigmoid()\n    )\n    (embed_positions): SinusoidalPositionalEmbedding()\n  )\n)", "parameters": [["bert.embed_out", [35, 768]], ["bert.embed_out_bias", [35]], ["bert.embed_tokens.weight", [35, 768]], ["bert.layers.0.self_attn.bias_k", [1, 1, 768]], ["bert.layers.0.self_attn.bias_v", [1, 1, 768]], ["bert.layers.0.self_attn.k_proj.weight", [768, 768]], ["bert.layers.0.self_attn.k_proj.bias", [768]], ["bert.layers.0.self_attn.v_proj.weight", [768, 768]], ["bert.layers.0.self_attn.v_proj.bias", [768]], ["bert.layers.0.self_attn.q_proj.weight", [768, 768]], ["bert.layers.0.self_attn.q_proj.bias", [768]], ["bert.layers.0.self_attn.out_proj.weight", [768, 768]], ["bert.layers.0.self_attn.out_proj.bias", [768]], ["bert.layers.0.self_attn_layer_norm.weight", [768]], ["bert.layers.0.self_attn_layer_norm.bias", [768]], ["bert.layers.0.fc1.weight", [3072, 768]], ["bert.layers.0.fc1.bias", [3072]], ["bert.layers.0.fc2.weight", [768, 3072]], ["bert.layers.0.fc2.bias", [768]], ["bert.layers.0.final_layer_norm.weight", [768]], ["bert.layers.0.final_layer_norm.bias", [768]], ["bert.layers.1.self_attn.bias_k", [1, 1, 768]], ["bert.layers.1.self_attn.bias_v", [1, 1, 768]], ["bert.layers.1.self_attn.k_proj.weight", [768, 768]], ["bert.layers.1.self_attn.k_proj.bias", [768]], ["bert.layers.1.self_attn.v_proj.weight", [768, 768]], ["bert.layers.1.self_attn.v_proj.bias", [768]], ["bert.layers.1.self_attn.q_proj.weight", [768, 768]], ["bert.layers.1.self_attn.q_proj.bias", [768]], ["bert.layers.1.self_attn.out_proj.weight", [768, 768]], ["bert.layers.1.self_attn.out_proj.bias", [768]], ["bert.layers.1.self_attn_layer_norm.weight", [768]], ["bert.layers.1.self_attn_layer_norm.bias", [768]], ["bert.layers.1.fc1.weight", [3072, 768]], ["bert.layers.1.fc1.bias", [3072]], ["bert.layers.1.fc2.weight", [768, 3072]], ["bert.layers.1.fc2.bias", [768]], ["bert.layers.1.final_layer_norm.weight", [768]], ["bert.layers.1.final_layer_norm.bias", [768]], ["bert.layers.2.self_attn.bias_k", [1, 1, 768]], ["bert.layers.2.self_attn.bias_v", [1, 1, 768]], ["bert.layers.2.self_attn.k_proj.weight", [768, 768]], ["bert.layers.2.self_attn.k_proj.bias", [768]], ["bert.layers.2.self_attn.v_proj.weight", [768, 768]], ["bert.layers.2.self_attn.v_proj.bias", [768]], ["bert.layers.2.self_attn.q_proj.weight", [768, 768]], ["bert.layers.2.self_attn.q_proj.bias", [768]], ["bert.layers.2.self_attn.out_proj.weight", [768, 768]], ["bert.layers.2.self_attn.out_proj.bias", [768]], ["bert.layers.2.self_attn_layer_norm.weight", [768]], ["bert.layers.2.self_attn_layer_norm.bias", [768]], ["bert.layers.2.fc1.weight", [3072, 768]], ["bert.layers.2.fc1.bias", [3072]], ["bert.layers.2.fc2.weight", [768, 3072]], ["bert.layers.2.fc2.bias", [768]], ["bert.layers.2.final_layer_norm.weight", [768]], ["bert.layers.2.final_layer_norm.bias", [768]], ["bert.layers.3.self_attn.bias_k", [1, 1, 768]], ["bert.layers.3.self_attn.bias_v", [1, 1, 768]], ["bert.layers.3.self_attn.k_proj.weight", [768, 768]], ["bert.layers.3.self_attn.k_proj.bias", [768]], ["bert.layers.3.self_attn.v_proj.weight", [768, 768]], ["bert.layers.3.self_attn.v_proj.bias", [768]], ["bert.layers.3.self_attn.q_proj.weight", [768, 768]], ["bert.layers.3.self_attn.q_proj.bias", [768]], ["bert.layers.3.self_attn.out_proj.weight", [768, 768]], ["bert.layers.3.self_attn.out_proj.bias", [768]], ["bert.layers.3.self_attn_layer_norm.weight", [768]], ["bert.layers.3.self_attn_layer_norm.bias", [768]], ["bert.layers.3.fc1.weight", [3072, 768]], ["bert.layers.3.fc1.bias", [3072]], ["bert.layers.3.fc2.weight", [768, 3072]], ["bert.layers.3.fc2.bias", [768]], ["bert.layers.3.final_layer_norm.weight", [768]], ["bert.layers.3.final_layer_norm.bias", [768]], ["bert.layers.4.self_attn.bias_k", [1, 1, 768]], ["bert.layers.4.self_attn.bias_v", [1, 1, 768]], ["bert.layers.4.self_attn.k_proj.weight", [768, 768]], ["bert.layers.4.self_attn.k_proj.bias", [768]], ["bert.layers.4.self_attn.v_proj.weight", [768, 768]], ["bert.layers.4.self_attn.v_proj.bias", [768]], ["bert.layers.4.self_attn.q_proj.weight", [768, 768]], ["bert.layers.4.self_attn.q_proj.bias", [768]], ["bert.layers.4.self_attn.out_proj.weight", [768, 768]], ["bert.layers.4.self_attn.out_proj.bias", [768]], ["bert.layers.4.self_attn_layer_norm.weight", [768]], ["bert.layers.4.self_attn_layer_norm.bias", [768]], ["bert.layers.4.fc1.weight", [3072, 768]], ["bert.layers.4.fc1.bias", [3072]], ["bert.layers.4.fc2.weight", [768, 3072]], ["bert.layers.4.fc2.bias", [768]], ["bert.layers.4.final_layer_norm.weight", [768]], ["bert.layers.4.final_layer_norm.bias", [768]], ["bert.layers.5.self_attn.bias_k", [1, 1, 768]], ["bert.layers.5.self_attn.bias_v", [1, 1, 768]], ["bert.layers.5.self_attn.k_proj.weight", [768, 768]], ["bert.layers.5.self_attn.k_proj.bias", [768]], ["bert.layers.5.self_attn.v_proj.weight", [768, 768]], ["bert.layers.5.self_attn.v_proj.bias", [768]], ["bert.layers.5.self_attn.q_proj.weight", [768, 768]], ["bert.layers.5.self_attn.q_proj.bias", [768]], ["bert.layers.5.self_attn.out_proj.weight", [768, 768]], ["bert.layers.5.self_attn.out_proj.bias", [768]], ["bert.layers.5.self_attn_layer_norm.weight", [768]], ["bert.layers.5.self_attn_layer_norm.bias", [768]], ["bert.layers.5.fc1.weight", [3072, 768]], ["bert.layers.5.fc1.bias", [3072]], ["bert.layers.5.fc2.weight", [768, 3072]], ["bert.layers.5.fc2.bias", [768]], ["bert.layers.5.final_layer_norm.weight", [768]], ["bert.layers.5.final_layer_norm.bias", [768]], ["bert.contact_head.regression.weight", [1, 72]], ["bert.contact_head.regression.bias", [1]]], "output_shape": [[16, 768], [16, 768]], "num_parameters": [26880, 35, 26880, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 768, 768, 589824, 768, 589824, 768, 589824, 768, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 72, 1]}], "edges": []}